#!/usr/bin/env python3
"""
KusiAI v1.323 — All‑in‑One AI Model
===================================

An end‑to‑end, batteries‑included reference implementation for a text
classification model with:

• Config system (YAML or CLI)
• Data ingestion (CSV/JSONL/plain text folders)
• Tokenization + vocabulary building
• PyTorch model (Transformer encoder)
• Training loop with mixed precision, gradient clipping, early stopping
• Evaluation (accuracy, precision, recall, F1, confusion matrix)
• Checkpointing + experiment logging
• Inference (batch + single)
• Export to TorchScript and ONNX
• Simple FastAPI server for realtime inference
• Clean CLI commands (train/eval/predict/serve/export)

Notes
-----
- Designed to be a single self‑contained file. You can split it later into a proper package.
- Only requires: torch, pydantic, fastapi, uvicorn, numpy, scikit-learn, PyYAML (optional), tqdm.
- If optional deps are missing (fastapi/uvicorn/yaml/sklearn), the app degrades gracefully.
- Dataset format: either a CSV/JSONL with columns [text,label] or a folder with subfolders per label containing .txt files.

Author: you + ChatGPT
Version: 1.323
License: MIT
"""
from __future__ import annotations
import os
import sys
import io
import re
import math
import time
import json
import random
import shutil
import signal
import string
import pathlib
import argparse
import dataclasses
from dataclasses import dataclass, asdict
from typing import List, Tuple, Dict, Optional, Iterable

try:
    import yaml  # type: ignore
except Exception:  # pragma: no cover
    yaml = None

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

# Optional deps
try:
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
except Exception:  # pragma: no cover
    accuracy_score = None
    precision_recall_fscore_support = None
    confusion_matrix = None

# FastAPI is optional; only needed for `serve`
try:
    from fastapi import FastAPI
    from pydantic import BaseModel
    import uvicorn
except Exception:  # pragma: no cover
    FastAPI = None
    BaseModel = object  # type: ignore
    uvicorn = None

# -------------------------
# Utilities
# -------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

def set_deterministic():
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def human_time(seconds: float) -> str:
    m, s = divmod(int(seconds), 60)
    h, m = divmod(m, 60)
    return f"{h:d}h {m:d}m {s:d}s"


def save_json(obj, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


def load_json(path: str):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

# -------------------------
# Config
# -------------------------
@dataclass
class ModelConfig:
    model_name: str = "kusiAI"
    version: str = "1.323"
    vocab_size: int = 30000
    max_len: int = 256
    d_model: int = 256
    n_heads: int = 4
    n_layers: int = 4
    dim_ff: int = 512
    dropout: float = 0.1

@dataclass
class TrainConfig:
    lr: float = 3e-4
    batch_size: int = 32
    num_epochs: int = 8
    weight_decay: float = 0.01
    warmup_steps: int = 200
    grad_clip: float = 1.0
    fp16: bool = True
    early_stopping_patience: int = 3

@dataclass
class DataConfig:
    train_path: str = "data/train.csv"  # csv/jsonl/folder
    val_path: Optional[str] = None
    test_path: Optional[str] = None
    text_key: str = "text"
    label_key: str = "label"
    min_freq: int = 2

@dataclass
class RuntimeConfig:
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    workdir: str = "runs/kusiAI_v1.323"
    num_workers: int = 2
    seed: int = SEED

@dataclass
class Config:
    model: ModelConfig = ModelConfig()
    train: TrainConfig = TrainConfig()
    data: DataConfig = DataConfig()
    run: RuntimeConfig = RuntimeConfig()

    @staticmethod
    def load(path: Optional[str]) -> "Config":
        if path is None:
            return Config()
        if yaml is None:
            raise RuntimeError("PyYAML not installed; cannot load YAML config")
        with open(path, 'r', encoding='utf-8') as f:
            raw = yaml.safe_load(f)
        # shallow merge
        def merge(dc, d):
            for k, v in d.items():
                if hasattr(dc, k):
                    sub = getattr(dc, k)
                    if dataclasses.is_dataclass(sub) and isinstance(v, dict):
                        merge(sub, v)
                    else:
                        setattr(dc, k, v)
        cfg = Config()
        merge(cfg, raw)
        return cfg

# -------------------------
# Simple tokenizer & vocab
# -------------------------
class BasicTokenizer:
    def __init__(self):
        self.pattern = re.compile(r"\w+|[^\w\s]", re.UNICODE)
    def __call__(self, text: str) -> List[str]:
        text = text.lower()
        return self.pattern.findall(text)

PAD, UNK, BOS, EOS = "<pad>", "<unk>", "<bos>", "<eos>"

class Vocab:
    def __init__(self, min_freq=2, max_size=30000):
        self.min_freq = min_freq
        self.max_size = max_size
        self.freqs: Dict[str, int] = {}
        self.itos: List[str] = []
        self.stoi: Dict[str, int] = {}

    def build(self, texts: Iterable[str]):
        tok = BasicTokenizer()
        for t in texts:
            for w in tok(t):
                self.freqs[w] = self.freqs.get(w, 0) + 1
        # special tokens
        specials = [PAD, UNK, BOS, EOS]
        vocab = specials + [w for w, c in sorted(self.freqs.items(), key=lambda x: (-x[1], x[0])) if c >= self.min_freq]
        if len(vocab) > self.max_size:
            vocab = vocab[: self.max_size]
        self.itos = vocab
        self.stoi = {w: i for i, w in enumerate(self.itos)}

    def __len__(self):
        return len(self.itos)

    def encode(self, text: str, max_len: int) -> List[int]:
        tok = BasicTokenizer()
        ids = [self.stoi.get(w, self.stoi.get(UNK, 1)) for w in tok(text)]
        ids = [self.stoi.get(BOS, 2)] + ids + [self.stoi.get(EOS, 3)]
        if len(ids) < max_len:
            ids = ids + [self.stoi.get(PAD, 0)] * (max_len - len(ids))
        else:
            ids = ids[:max_len]
        return ids

# -------------------------
# Data loading
# -------------------------
class TextClassificationDataset(Dataset):
    def __init__(self, records: List[Tuple[str, int]], vocab: Vocab, max_len: int):
        self.records = records
        self.vocab = vocab
        self.max_len = max_len
    def __len__(self):
        return len(self.records)
    def __getitem__(self, idx):
        text, label = self.records[idx]
        ids = torch.tensor(self.vocab.encode(text, self.max_len), dtype=torch.long)
        return ids, torch.tensor(label, dtype=torch.long)


def read_folder_dataset(path: str) -> Tuple[List[str], List[int], List[str]]:
    classes = sorted([d.name for d in pathlib.Path(path).iterdir() if d.is_dir()])
    label2id = {c: i for i, c in enumerate(classes)}
    texts, labels = [], []
    for c in classes:
        for fp in pathlib.Path(path, c).glob("**/*.txt"):
            txt = fp.read_text(encoding='utf-8', errors='ignore')
            texts.append(txt)
            labels.append(label2id[c])
    return texts, labels, classes


def read_tabular_dataset(path: str, text_key: str, label_key: str) -> Tuple[List[str], List[int], List[str]]:
    texts, labels = [], []
    classes: List[str] = []
    ext = pathlib.Path(path).suffix.lower()
    def add_label(l):
        nonlocal classes
        if l not in classes:
            classes.append(l)
        return classes.index(l)
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if not line.strip():
                continue
            if ext == '.jsonl':
                obj = json.loads(line)
            else:
                # simplistic CSV (comma in text may break; use JSONL for safety)
                parts = [p.strip() for p in line.split(',')]
                if len(parts) < 2:
                    # header or malformed line
                    if text_key in line and label_key in line:
                        continue
                    else:
                        # try tab
                        parts = [p.strip() for p in line.split('\t')]
                        if len(parts) < 2:
                            continue
                obj = {text_key: parts[0], label_key: parts[1]}
            t = str(obj[text_key])
            l = str(obj[label_key])
            texts.append(t)
            labels.append(add_label(l))
    return texts, labels, classes


def load_dataset(path: str, text_key: str, label_key: str) -> Tuple[List[str], List[int], List[str]]:
    p = pathlib.Path(path)
    if p.is_dir():
        return read_folder_dataset(path)
    return read_tabular_dataset(path, text_key, label_key)

